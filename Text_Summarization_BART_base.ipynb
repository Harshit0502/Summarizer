{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 32267,
          "sourceType": "datasetVersion",
          "datasetId": 24984
        },
        {
          "sourceId": 6539218,
          "sourceType": "datasetVersion",
          "datasetId": 3780376
        },
        {
          "sourceId": 10093095,
          "sourceType": "datasetVersion",
          "datasetId": 1977878
        },
        {
          "sourceId": 13378480,
          "sourceType": "datasetVersion",
          "datasetId": 8488048
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshit0502/Summarizer/blob/main/Text_Summarization_BART_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "pariza_bbc_news_summary_path = kagglehub.dataset_download('pariza/bbc-news-summary')\n",
        "gpreda_bbc_news_path = kagglehub.dataset_download('gpreda/bbc-news')\n",
        "harshitmahour_bbc_csv_path = kagglehub.dataset_download('harshitmahour/bbc-csv')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kIbDeGyc6Hn",
        "outputId": "c96fccdd-a78d-4b6c-a528-fbdbc48e4c80"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'bbc-news-summary' dataset.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Imports the library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import warnings\n",
        "import os"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:35:09.13673Z",
          "iopub.execute_input": "2025-10-14T06:35:09.13702Z",
          "iopub.status.idle": "2025-10-14T06:35:11.144623Z",
          "shell.execute_reply.started": "2025-10-14T06:35:09.136989Z",
          "shell.execute_reply": "2025-10-14T06:35:11.143662Z"
        },
        "trusted": true,
        "id": "k-8Rw7Izc6Hq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "paths = os.listdir('../input/bbc-news-summary/BBC News Summary/News Articles')\n",
        "articles_path = '../input/bbc-news-summary/BBC News Summary/News Articles/'\n",
        "summaries_path = '../input/bbc-news-summary/BBC News Summary/Summaries/'\n",
        "\n",
        "articles = []\n",
        "summaries = []\n",
        "file_arr = []\n",
        "\n",
        "for path in paths:\n",
        "    files = os.listdir(articles_path + path)\n",
        "    for file in files:\n",
        "        article_file_path = articles_path + path + '/' + file\n",
        "        summary_file_path = summaries_path + path + '/' + file\n",
        "        try:\n",
        "            with open (article_file_path,'r') as f:\n",
        "                articles.append('.'.join([line.rstrip() for line in f.readlines()]))\n",
        "            with open (summary_file_path,'r') as f:\n",
        "                summaries.append('.'.join([line.rstrip() for line in f.readlines()]))\n",
        "            file_arr.append(path + '/' + file)\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:35:11.146267Z",
          "iopub.execute_input": "2025-10-14T06:35:11.146553Z",
          "iopub.status.idle": "2025-10-14T06:35:32.373336Z",
          "shell.execute_reply.started": "2025-10-14T06:35:11.146532Z",
          "shell.execute_reply": "2025-10-14T06:35:32.372521Z"
        },
        "trusted": true,
        "id": "SXcaPgkPc6Hr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 200)\n",
        "df = pd.DataFrame({'path':file_arr,'article': articles,'summary':summaries})\n",
        "df.head(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:35:32.398542Z",
          "iopub.execute_input": "2025-10-14T06:35:32.398877Z",
          "iopub.status.idle": "2025-10-14T06:35:32.435968Z",
          "shell.execute_reply.started": "2025-10-14T06:35:32.398849Z",
          "shell.execute_reply": "2025-10-14T06:35:32.435223Z"
        },
        "trusted": true,
        "id": "pbV_MaJfc6Hr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis and EDA"
      ],
      "metadata": {
        "id": "pACb2G9Qc6Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding word counts  to both article and the summary to get a better understanding of the text size"
      ],
      "metadata": {
        "id": "l-h4hdWuc6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count the number of words in a text\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "df['wordcnt_article'] = df['article'].apply(count_words)\n",
        "df['wordcnt_summary'] = df['summary'].apply(count_words)\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:35:44.444295Z",
          "iopub.execute_input": "2025-10-14T06:35:44.444853Z",
          "iopub.status.idle": "2025-10-14T06:35:44.518391Z",
          "shell.execute_reply.started": "2025-10-14T06:35:44.444824Z",
          "shell.execute_reply": "2025-10-14T06:35:44.517543Z"
        },
        "trusted": true,
        "id": "J0LufWamc6Ht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:35:53.110996Z",
          "iopub.execute_input": "2025-10-14T06:35:53.111806Z",
          "iopub.status.idle": "2025-10-14T06:35:53.130768Z",
          "shell.execute_reply.started": "2025-10-14T06:35:53.111776Z",
          "shell.execute_reply": "2025-10-14T06:35:53.130001Z"
        },
        "trusted": true,
        "id": "6JQTAiqfc6Ht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:08.872441Z",
          "iopub.execute_input": "2025-10-14T06:36:08.873118Z",
          "iopub.status.idle": "2025-10-14T06:36:08.895233Z",
          "shell.execute_reply.started": "2025-10-14T06:36:08.873088Z",
          "shell.execute_reply": "2025-10-14T06:36:08.894345Z"
        },
        "trusted": true,
        "id": "P2v2YjIxc6Ht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = df.select_dtypes(include=['int64'])\n",
        "\n",
        "# Create histograms for all numerical columns\n",
        "numerical_columns.hist(bins=30, figsize=(8, 4))  # Adjust 'bins' and 'figsize' as needed\n",
        "plt.tight_layout()  # Ensure proper spacing between plots\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:14.37336Z",
          "iopub.execute_input": "2025-10-14T06:36:14.374193Z",
          "iopub.status.idle": "2025-10-14T06:36:14.953975Z",
          "shell.execute_reply.started": "2025-10-14T06:36:14.374162Z",
          "shell.execute_reply": "2025-10-14T06:36:14.953049Z"
        },
        "trusted": true,
        "id": "_tzxAzw7c6Hu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['wordcnt_article'] < 400]\n",
        "filtered_df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:21.490399Z",
          "iopub.execute_input": "2025-10-14T06:36:21.490717Z",
          "iopub.status.idle": "2025-10-14T06:36:21.508132Z",
          "shell.execute_reply.started": "2025-10-14T06:36:21.490692Z",
          "shell.execute_reply": "2025-10-14T06:36:21.507131Z"
        },
        "trusted": true,
        "id": "zcrCqOudc6Hu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the distributions again"
      ],
      "metadata": {
        "id": "wrJxFlQjc6Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = filtered_df.select_dtypes(include=['int64'])\n",
        "\n",
        "# Create histograms for all numerical columns\n",
        "numerical_columns.hist(bins=30, figsize=(8, 4))  # Adjust 'bins' and 'figsize' as needed\n",
        "plt.tight_layout()  # Ensure proper spacing between plots\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:29.570058Z",
          "iopub.execute_input": "2025-10-14T06:36:29.570594Z",
          "iopub.status.idle": "2025-10-14T06:36:30.104988Z",
          "shell.execute_reply.started": "2025-10-14T06:36:29.570565Z",
          "shell.execute_reply": "2025-10-14T06:36:30.104081Z"
        },
        "trusted": true,
        "id": "lp6AaM8vc6Hu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:35.922555Z",
          "iopub.execute_input": "2025-10-14T06:36:35.922886Z",
          "iopub.status.idle": "2025-10-14T06:36:35.933051Z",
          "shell.execute_reply.started": "2025-10-14T06:36:35.92286Z",
          "shell.execute_reply": "2025-10-14T06:36:35.932183Z"
        },
        "trusted": true,
        "id": "WdXWYBTac6Hu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "F_N2GgD7c6Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns from the DataFrame\n",
        "filtered_df = filtered_df.drop(['wordcnt_article', 'wordcnt_summary', 'path'], axis=1)\n",
        "\n",
        "# Check the information of the updated DataFrame\n",
        "filtered_df.info()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:46.222102Z",
          "iopub.execute_input": "2025-10-14T06:36:46.222949Z",
          "iopub.status.idle": "2025-10-14T06:36:46.235694Z",
          "shell.execute_reply.started": "2025-10-14T06:36:46.222917Z",
          "shell.execute_reply": "2025-10-14T06:36:46.234666Z"
        },
        "trusted": true,
        "id": "xTAdCrDDc6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of rows to sample (30% of the original data)\n",
        "sample_size = int(0.3 * len(filtered_df))\n",
        "\n",
        "# Sample 30% of the data\n",
        "sampled_df = filtered_df.sample(n=sample_size, random_state=42)  # You can choose any random_state value for reproducibility\n",
        "sampled_df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:36:54.196467Z",
          "iopub.execute_input": "2025-10-14T06:36:54.197392Z",
          "iopub.status.idle": "2025-10-14T06:36:54.206588Z",
          "shell.execute_reply.started": "2025-10-14T06:36:54.19736Z",
          "shell.execute_reply": "2025-10-14T06:36:54.205685Z"
        },
        "trusted": true,
        "id": "ezzL-4c0c6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df=sampled_df\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:37:12.975682Z",
          "iopub.execute_input": "2025-10-14T06:37:12.975987Z",
          "iopub.status.idle": "2025-10-14T06:37:12.979947Z",
          "shell.execute_reply.started": "2025-10-14T06:37:12.975965Z",
          "shell.execute_reply": "2025-10-14T06:37:12.978969Z"
        },
        "trusted": true,
        "id": "73ObT4Cec6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:37:18.972665Z",
          "iopub.execute_input": "2025-10-14T06:37:18.972986Z",
          "iopub.status.idle": "2025-10-14T06:37:18.981916Z",
          "shell.execute_reply.started": "2025-10-14T06:37:18.972962Z",
          "shell.execute_reply": "2025-10-14T06:37:18.981121Z"
        },
        "trusted": true,
        "id": "E7SFqcPwc6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:37:24.659079Z",
          "iopub.execute_input": "2025-10-14T06:37:24.659459Z",
          "iopub.status.idle": "2025-10-14T06:37:35.661964Z",
          "shell.execute_reply.started": "2025-10-14T06:37:24.659419Z",
          "shell.execute_reply": "2025-10-14T06:37:35.660999Z"
        },
        "trusted": true,
        "id": "vXD5WZbXc6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:37:37.089477Z",
          "iopub.execute_input": "2025-10-14T06:37:37.08985Z",
          "iopub.status.idle": "2025-10-14T06:37:48.953837Z",
          "shell.execute_reply.started": "2025-10-14T06:37:37.089819Z",
          "shell.execute_reply": "2025-10-14T06:37:48.952907Z"
        },
        "trusted": true,
        "id": "eJtdpIblc6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the device for GPU usage (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "max_length = 512  # Maximum sequence length\n",
        "\n",
        "def tokenize_text(text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length', return_attention_mask=True)\n",
        "    return inputs.to(device)  # Move the tokenized inputs to the GPU\n",
        "\n",
        "def tokenize_summary(text):\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=280, truncation=True, padding='max_length', return_attention_mask=True)\n",
        "    return inputs.to(device)  # Move the tokenized summaries to the GPU\n",
        "\n",
        "\n",
        "df['TokenizedText'] = df['article'].apply(tokenize_text)\n",
        "df['TokenizedSummary'] = df['summary'].apply(tokenize_summary)\n",
        "\n",
        "# Split your data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert tokenized data to PyTorch tensors\n",
        "X_train = torch.stack([seq.squeeze() for seq in train_df['TokenizedText']])\n",
        "Y_train = torch.stack([seq.squeeze() for seq in train_df['TokenizedSummary']])\n",
        "X_test = torch.stack([seq.squeeze() for seq in test_df['TokenizedText']])\n",
        "Y_test = torch.stack([seq.squeeze() for seq in test_df['TokenizedSummary']])\n",
        "\n",
        "# Define a DataLoader for batching data\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:37:53.057799Z",
          "iopub.execute_input": "2025-10-14T06:37:53.058134Z",
          "iopub.status.idle": "2025-10-14T06:37:56.465027Z",
          "shell.execute_reply.started": "2025-10-14T06:37:53.058107Z",
          "shell.execute_reply": "2025-10-14T06:37:56.464231Z"
        },
        "trusted": true,
        "id": "8eCr8eoYc6Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BART model\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
        "\n",
        "# Create a GradScaler for mixed-precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Define hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move the model to the GPU\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=len(train_dataloader) * 10)  # Add learning rate scheduler\n",
        "early_stopping_rounds = 2\n",
        "best_rouge_score = -1\n",
        "current_round = 0\n",
        "\n",
        "# Define gradient accumulation steps\n",
        "accumulation_steps = 20  # You can adjust this number\n",
        "\n",
        "def train(model, dataloader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "        inputs = batch[0].to(device)  # Move the input batch to the GPU\n",
        "        attention_mask = (inputs != 0).float().to(device)  # Create attention mask\n",
        "        targets = batch[1].to(device)  # Move the target batch to the GPU\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=inputs, attention_mask=attention_mask, decoder_input_ids=targets, labels=targets)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Perform gradient accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            # Update gradients and optimizer once every accumulation_steps\n",
        "            clip_grad_norm_(model.parameters(), max_norm=1.0)  # Optional gradient clipping\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def calculate_rouge1_precision(logits, targets):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    rouge1_precision = 0.0\n",
        "    num_samples = len(logits)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        predicted_ids = logits[i].cpu().numpy()\n",
        "        target_ids = targets[i].cpu().numpy()\n",
        "\n",
        "        # Convert token IDs to strings\n",
        "        predicted_text = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
        "        target_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Calculate ROUGE-1 precision\n",
        "        scores = scorer.score(predicted_text, target_text)\n",
        "        rouge1_precision += scores['rouge1'].precision\n",
        "\n",
        "    return rouge1_precision / num_samples\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):  # Change the number of epochs as needed\n",
        "    train_loss = train(model, train_dataloader, optimizer, scheduler)\n",
        "    print(f\"Epoch {epoch+1}/{2}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:38:17.386189Z",
          "iopub.execute_input": "2025-10-14T06:38:17.386738Z",
          "iopub.status.idle": "2025-10-14T06:39:07.682104Z",
          "shell.execute_reply.started": "2025-10-14T06:38:17.386707Z",
          "shell.execute_reply": "2025-10-14T06:39:07.681193Z"
        },
        "trusted": true,
        "id": "48e3o9JHc6Hw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    test_articles = []\n",
        "    actual_summaries = []\n",
        "    predicted_summaries = []\n",
        "    rouge1_precision_scores = []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating Test\"):\n",
        "            inputs = batch[0].to(device)\n",
        "            attention_mask = (inputs != 0).float().to(device)\n",
        "            targets = batch[1].to(device)\n",
        "            outputs = model.generate(input_ids=inputs, attention_mask=attention_mask, max_length=150, num_beams=17, length_penalty=2.0, early_stopping=False)\n",
        "\n",
        "            for output, target, input_text in zip(outputs, targets, inputs):\n",
        "                # Calculate ROUGE-1 precision for each sample\n",
        "                output_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "                target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
        "                rouge_scores = scorer.score(output_text, target_text)\n",
        "                rouge1_precision_scores.append(rouge_scores['rouge1'].precision)\n",
        "\n",
        "                # Append tokenized text, actual summaries, and predicted summaries\n",
        "                test_articles.append(tokenizer.decode(input_text, skip_special_tokens=True))\n",
        "                actual_summaries.append(target_text)\n",
        "                predicted_summaries.append(output_text)\n",
        "\n",
        "    return test_articles, actual_summaries, predicted_summaries, rouge1_precision_scores\n",
        "\n",
        "# Evaluate the model\n",
        "test_articles, actual_summaries, predicted_summaries, rouge1_precision_scores = evaluate(model, test_dataloader)\n",
        "\n",
        "# Create a dictionary with the extracted data\n",
        "data = {\n",
        "    'Article': test_articles,\n",
        "    'Actual Summary': actual_summaries,\n",
        "    'Predicted Summary': predicted_summaries,\n",
        "    'ROUGE-1 Precision': rouge1_precision_scores,\n",
        "}\n",
        "\n",
        "# Create a Pandas DataFrame from the dictionary\n",
        "results_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results_df.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:39:07.683788Z",
          "iopub.execute_input": "2025-10-14T06:39:07.68406Z",
          "iopub.status.idle": "2025-10-14T06:40:47.806601Z",
          "shell.execute_reply.started": "2025-10-14T06:39:07.684037Z",
          "shell.execute_reply": "2025-10-14T06:40:47.805541Z"
        },
        "trusted": true,
        "id": "zrQDrSoAc6Hw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "results_df.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-14T06:40:47.8218Z",
          "iopub.execute_input": "2025-10-14T06:40:47.822118Z",
          "iopub.status.idle": "2025-10-14T06:40:47.84188Z",
          "shell.execute_reply.started": "2025-10-14T06:40:47.822095Z",
          "shell.execute_reply": "2025-10-14T06:40:47.841091Z"
        },
        "trusted": true,
        "id": "6Xa0UMMtc6Hw"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}